apiVersion: llama.x-k8s.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastackdistribution-sample
spec:
  replicas: 1
  server:
    containerSpec:
      env:
        - name: INFERENCE_MODEL
          value: 'llama3.2:1b'
        - name: OLLAMA_URL
          value: 'http://ollama-server-service.ollama-dist.svc.cluster.local:11434'
      name: llama-stack
    distribution:
      # name: ollama
    podOverrides:
      volumeMounts:
        - mountPath: /root/.llama
          name: llama-storage
      volumes:
        - emptyDir: {}
          name: llama-storage
